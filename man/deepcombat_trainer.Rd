% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepcombat_trainer.R
\name{deepcombat_trainer}
\alias{deepcombat_trainer}
\title{DeepComBat CVAE Trainer}
\usage{
deepcombat_trainer(
  setup,
  train_epochs = c(5, 30, 5),
  anneal_rate = 5,
  lambda = 0.1,
  optimizer = NULL,
  verbose = FALSE
)
}
\arguments{
\item{setup}{a deepcombat_setup_object generated by deepcombat_setup function.}

\item{train_epochs}{a vector of length 3 specifying the number of epochs for pre-training, cyclic annealing, and post-annealing training, respectively. Larger datasets may converge with fewer total epochs while smaller datasets may require more. Default is c(5, 30, 5).}

\item{anneal_rate}{an integer indicating the number of epochs per cyclic annealing cycle. Default is 5, indicating KL-divergence weight increases linearly from 0 to lambda over 5 epochs.}

\item{lambda}{a scalar indicating the desired final weight for the KL divergence loss term during training. Default is 0.1.}

\item{optimizer}{an optimizer object from the torch package to be used during training. Default is NULL, which uses the Adam optimizer provided by 'setup'.}

\item{verbose}{a logical value indicating whether to print progress updates during training. Default is FALSE.}
}
\value{
Returns the trained CVAE model.
}
\description{
This function trains the DeepComBat conditional variational autoencoder (CVAE). It first pre-trains a standard autoencoder without a KL-divergence loss function component.
Then, it performs a series of cyclic annealing cycles to gradually increase KL-divergence weighting to the desired KL-divergence while minimizing risk of posterior collapse.
Finally, it performs a few post-training epochs at the desired KL-divergence weight.
}
\examples{
\dontrun{
# If Adam optimizer with default settings is desired
cvae_model <- deepcombat_trainer(setup_obj, train_epochs = c(5, 30, 5), 
anneal_rate = 5, lambda = 0.1, optimizer = NULL, verbose = TRUE)

# If alternate optimizer, such as SGD, is desired. 
# First parameter of alternate optimizers should be setup$cvae$parameters
cvae_model <- deepcombat_trainer(setup_obj, train_epochs = c(5, 30, 5), 
anneal_rate = 5, lambda = 0.1, 
optimizer = optim_sgd(setup$cvae$parameters, lr = 0.1), verbose = TRUE)
}
}
